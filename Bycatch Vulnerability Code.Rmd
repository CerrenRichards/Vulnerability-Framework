---
title: "Supplementary Code"
subtitle: "Speciesâ€™ traits determine seabird bycatch vulnerability in global fisheries"
author: "*Cerren Richards, Robert S.C. Cooke, Diana E. Bowler, Kristina Boerder, Amanda E. Bates*"
date: '16-07-2020'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/cerridwenr/Desktop/Masters Thesis /Fisheries")

```


###___________________________________________________________________________

### Contents

1. Preparing the Global Fishing Watch data 
    * Sum the data by gear type, latitude and longitude
    * Further reduce the dataframe size for each gear type
    * Visualise the fishing effort data
    
2. Preparing BirdLife International species distribution polygons  
    * Import and subset the seabird polygons 
    * Create a species presence-absence matrix
    
3. Quantify fishing overlap and intensity for all species

4. Quantify gear-specific bycatch vulnerability
    * Longline Vulnerability
    * Trawl Vulnerability
    * Purse Seine Vulnerability
    * Building vulnerability score dataframes
    
5. Importing species' threat data from the IUCN
    * Subset species at risk to bycatch

6. Assigning species to vulnerability classes

7. Create tables and figures
    * Table for mean vulnerability scores
    * Table for top five most vulnerable species

###___________________________________________________________________________

## **1. Preparing the Global Fishing Watch data** 

Here we use the 2015 - 2018 Global Fishing Watch data. These data were provided as the monthly sum of fishing hours per gear type per 1x1 degree cell. The data points are located in the southwestern corner of a respective cell. To centre the data within the cell, all points will need to be shifted by 0.5 degree North and East.

Fishing effort is available for the time period 2012 to 2016 from https://globalfishingwatch.org/. To access to provisional 2017 and 2018 data, contact research@globalfishingwatch.org.

### *Sum the data by gear type, latitude and longitude*

The fishing effort .csv files are very large, so we read each year in individually and reduce the data to prevent R from crashing.

```{r, eval = FALSE}
library(dplyr)


### 2015 ###


# read in the 2015 data 
fish15 <- read.csv("MUN_fishing_effort_2015.csv")

# Make the lat and long in the correct format
fish15$lat_bin <- (fish15$lat_bin)/100
fish15$lon_bin <- (fish15$lon_bin)/100

# Shift the lat long to the center of cells
fish15$lat_bin <- (fish15$lat_bin)+0.5
fish15$lon_bin <- (fish15$lon_bin)+0.5

# Subset the dataframe to only include three gear types
fish_gears_15<- filter(fish15, geartype %in% c("drifting_longlines", "trawlers", "purse_seines"))

remove(fish15) # remove to free space in the working environment 

# Sum the fishing_hours by gear type, latitude and longitude
sum_gear_15 <- fish_gears_15 %>% group_by(lat_bin, lon_bin, geartype) %>% 
                summarise(fishing_hours=sum(fishing_hours))

remove(fish_gears_15) # remove to free space

# Subset dataframes by gear type 
longline_15 <- sum_gear_15[ which(sum_gear_15$geartype=='drifting_longlines'), ]
trawler_15 <- sum_gear_15[ which(sum_gear_15$geartype=='trawlers'), ]
seine_15 <- sum_gear_15[ which(sum_gear_15$geartype=='purse_seines'), ]

# Save as a .rds files
saveRDS(longline_15, "longline_15.rds")
saveRDS(trawler_15, "trawler_15.rds")
saveRDS(seine_15, "seine_15.rds")

# Clear the R environment 
rm(list = ls())



### 2016 ###


# read in the 2016 data 
fish16 <- read.csv("MUN_fishing_effort_2016.csv")

# Make the lat and long in the correct format
fish16$lat_bin <- (fish16$lat_bin)/100
fish16$lon_bin <- (fish16$lon_bin)/100

# Shift the lat long to the center of cells
fish16$lat_bin <- (fish16$lat_bin)+0.5
fish16$lon_bin <- (fish16$lon_bin)+0.5

# Subset the dataframe to only include three gear types
fish_gears_16<- filter(fish16, geartype %in% c("drifting_longlines", "trawlers", "purse_seines"))

remove(fish16) # remove to free space

# Sum the fishing_hours by gear type, latitude and longitude
sum_gear_16 <- fish_gears_16 %>% group_by(lat_bin, lon_bin, geartype) %>% 
                summarise(fishing_hours=sum(fishing_hours))

remove(fish_gears_16) # remove to free space

# Subset dataframes by gear type 
longline_16 <- sum_gear_16[ which(sum_gear_16$geartype=='drifting_longlines'), ]
trawler_16 <- sum_gear_16[ which(sum_gear_16$geartype=='trawlers'), ]
seine_16 <- sum_gear_16[ which(sum_gear_16$geartype=='purse_seines'), ]

# Save as a .rds files
saveRDS(longline_16, "longline_16.rds")
saveRDS(trawler_16, "trawler_16.rds")
saveRDS(seine_16, "seine_16.rds")

# Clear the R environment 
rm(list = ls())


### 2017 ###

# read in the 2017 data 
fish17 <- read.csv("MUN_fishing_effort_2017.csv")

# Make the lat and long in the correct format
fish17$lat_bin <- (fish17$lat_bin)/100
fish17$lon_bin <- (fish17$lon_bin)/100

# Shift the lat long to the center of cells
fish17$lat_bin <- (fish17$lat_bin)+0.5
fish17$lon_bin <- (fish17$lon_bin)+0.5


# Subset the dataframe to only include three gear types
fish_gears_17<- filter(fish17, geartype %in% c("drifting_longlines", "trawlers", "purse_seines"))

remove(fish17) # remove to free space

# Sum the fishing_hours by gear type, latitude and longitude
sum_gear_17 <- fish_gears_17 %>% group_by(lat_bin, lon_bin, geartype) %>% 
                summarise(fishing_hours=sum(fishing_hours))

remove(fish_gears_17) # remove to free space

# Subset dataframes by gear type 
longline_17 <- sum_gear_17[ which(sum_gear_17$geartype=='drifting_longlines'), ]
trawler_17 <- sum_gear_17[ which(sum_gear_17$geartype=='trawlers'), ]
seine_17 <- sum_gear_17[ which(sum_gear_17$geartype=='purse_seines'), ]

# Save as a .rds files
saveRDS(longline_17, "longline_17.rds")
saveRDS(trawler_17, "trawler_17.rds")
saveRDS(seine_17, "seine_17.rds")

# Clear the R environment 
rm(list = ls())


### 2018 ###


# read in the 2018 data 
fish18 <- read.csv("MUN_fishing_effort_2018.csv")

# Make the lat and long in the correct format
fish18$lat_bin <- (fish18$lat_bin)/100
fish18$lon_bin <- (fish18$lon_bin)/100

# Shift the lat long to the center of cells
fish18$lat_bin <- (fish18$lat_bin)+0.5
fish18$lon_bin <- (fish18$lon_bin)+0.5

# Subset the dataframe to only include three gear types
fish18<- filter(fish18, geartype %in% c("drifting_longlines", "trawlers", "purse_seines"))


# Sum the fishing_hours by gear type, latitude and longitude
fish18 <- fish18 %>% group_by(lat_bin, lon_bin, geartype) %>% 
                summarise(fishing_hours=sum(fishing_hours))


# Subset dataframes by gear type 
longline_18 <- fish18[ which(fish18$geartype=='drifting_longlines'), ]
trawler_18 <- fish18[ which(fish18$geartype=='trawlers'), ]
seine_18 <- fish18[ which(fish18$geartype=='purse_seines'), ]

# Save as a .rds files
saveRDS(longline_18, "longline_18.rds")
saveRDS(trawler_18, "trawler_18.rds")
saveRDS(seine_18, "seine_18.rds")

rm(list = ls())
```



### *Further reduce the dataframe size for each gear type*

Here we combine all the gear type files for each year and further summaries the fishing hours by latitude and longitude.

```{r, eval = FALSE}

### SEINES ###


# Read in all of the seine .rds files
seine_15 <- readRDS("seine_15.rds") 
seine_16 <- readRDS("seine_16.rds") 
seine_17 <- readRDS("seine_17.rds") 
seine_18 <- readRDS("seine_18.rds")

# Bind all the files together
tot_seines <- rbind(seine_15, seine_16, seine_17, seine_18)

# Sum the number of hours for all seines per cell between 2015-2018
SUM_seines <- tot_seines %>% group_by(lat_bin, lon_bin, geartype) %>% 
                summarise(fishing_hours=sum(fishing_hours))

# Save the final seine dataframe
saveRDS(SUM_seines, "SUM_seines.rds")

# Clear the R environment 
rm(list = ls())


### TRAWLERS ###


# Read in all of the trawler .rds files
trawler_15 <- readRDS("trawler_15.rds") 
trawler_16 <- readRDS("trawler_16.rds") 
trawler_17 <- readRDS("trawler_17.rds") 
trawler_18 <- readRDS("trawler_18.rds")

# Bind all the files together
tot_trawlers <- rbind(trawler_15, trawler_16, trawler_17, trawler_18)

# Sum the number of hours for all trawlers per cell between 2015-2018
SUM_trawlers <- tot_trawlers %>% group_by(lat_bin, lon_bin, geartype) %>% 
  summarise(fishing_hours=sum(fishing_hours))

# Save the final trawler dataframe
saveRDS(SUM_trawlers, "SUM_trawlers.rds")

# Clear the R environment 
rm(list = ls())



### LONGLINES ###


# Read in all of the longline .rds files
longline_15 <- readRDS("longline_15.rds") 
longline_16 <- readRDS("longline_16.rds") 
longline_17 <- readRDS("longline_17.rds") 
longline_18 <- readRDS("longline_18.rds")

# Bind all the files together
tot_longlines <- rbind(longline_15, longline_16, longline_17, longline_18)

# Sum the number of hours for all longlines per cell between 2015-2018
tot_longlines <- tot_longlines %>% group_by(lat_bin, lon_bin, geartype) %>% 
  summarise(fishing_hours=sum(fishing_hours))

# Save the final longline dataframe
saveRDS(tot_longlines, "SUM_longlines.rds")

# Clear the R environment 
rm(list = ls())


```


### *Visualise the fishing effort data*

Note - This is takes a really long time to run for the longline data. 

```{r, eval = FALSE}

library(ggplot2);library(viridis); library(ggpubr); library(tidyverse);library(maps)


worldmap <- map_data("world") # extract map of the world 


### SEINES ###

seine <- readRDS('SUM_seines.rds')

# map seines
ggplot() +
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + 
  geom_point(aes(x = lon_bin, y = lat_bin, colour = fishing_hours), data = seine) + 
  scale_color_viridis(option = "D", guide = guide_colorbar
  (title.position = 'top', title.hjust = 0.5, label.hjust = 0.5,
  barheight = unit(2, units = "mm"), barwidth = unit(50, units = "mm"))) +
  geom_path(data = worldmap, aes(x = long, y = lat, group = group))+
  scale_y_continuous(expand = c(0,0), limits = c(-84, 85))+
  scale_x_continuous(expand = c(0,0), limits = c(-180, 180))+
  theme(legend.position = "bottom", legend.box = "vertical")+
  labs(colour = "Seine Fishing Hours") + 
  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+
  geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = "grey60")

remove(seine)



### LONGLINES ###


longline <- readRDS('SUM_longlines.rds')

ggplot() +
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point(aes(x = lon_bin, y = lat_bin, colour = fishing_hours), data = longline) + 
  scale_color_viridis(option = "D", guide = guide_colorbar
  (title.position = 'top', title.hjust = 0.5, label.hjust = 0.5,
  barheight = unit(2, units = "mm"), barwidth = unit(50, units = "mm"))) +
  geom_path(data = worldmap, aes(x = long, y = lat, group = group))+
  scale_y_continuous(expand = c(0,0), limits = c(-84, 85))+
  scale_x_continuous(expand = c(0,0), limits = c(-180, 180))+
  theme(legend.position = "bottom", legend.box = "vertical")+
  labs(colour = "Longline Fishing Hours") + 
  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+
  geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = "grey60")


remove(longline)


### TRAWL ###

trawlers <- readRDS('SUM_trawlers.rds')

ggplot() +
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point(aes(x = lon_bin, y = lat_bin, colour = fishing_hours), data = trawler) + 
  scale_color_viridis(option = "D", guide = guide_colorbar
  (title.position = 'top', title.hjust = 0.5, label.hjust = 0.5,
  barheight = unit(2, units = "mm"), barwidth = unit(50, units = "mm"))) +
  geom_path(data = worldmap, aes(x = long, y = lat, group = group))+
  scale_y_continuous(expand = c(0,0), limits = c(-84, 85))+
  scale_x_continuous(expand = c(0,0), limits = c(-180, 180))+
  theme(legend.position = "bottom", legend.box = "vertical")+
  labs(colour = "Trawl Fishing Hours") + 
  theme(axis.title.x=element_blank(), axis.title.y=element_blank())+
  geom_polygon(data = worldmap, aes(x = long, y = lat, group = group), fill = "grey60")


remove(trawlers)


```





## **2. Preparing BirdLife International species distribution polygons**


We downloaded the Bird of the World polygons from BirdLife International. These spatial polygons represent the coarse distributions that species likely occupy, and are presently the best available data for the ranges of all seabirds.

Bird distribution data are available upon request from http://datazone.birdlife.org/species/requestdis/.

### *Import and subset the seabird polygons* 

Here we subset the seabird polygons based on the list of seabirds from Richards et al. (2020). 

```{r, eval = FALSE}
library(rgdal); library(dplyr)

# Read in the Bird of the World (BOTW) data 
# This takes a long time because it is a file containing multiple distribution polygons for 9000 species
BOTW <- readOGR(dsn = "/Users/cerridwenr/Desktop/Masters Thesis /Chapter two/BOTW.gdb", layer = "All_Species")

# Read in the trait and seabird data from Richards et al. (2020)
traits <- read.csv("341_sb_traits.csv")

# Subset the BOTW dataset to only contain seabirds
sb <- subset(BOTW, SCINAME %in% traits$Binomial)

#save the SHAPE file as an RDS
saveRDS(sb, "BOTW_seabirds_341.rds")

```


### *Create a species presence-absence matrix*

Here we further subset the seabird polygons to only retain the extant, native, resident, breeding season and non-breeding season polygons. Second, we create a 1Â° resolution global presence-absence matrix based on the seabird distribution polygons. 

```{r, eval = FALSE}
library(letsR)

# .rds file that contains the subsetted seabird polygons from the large BOTW data file 
sb <- readRDS("BOTW_seabirds_341.rds") # note this file has different binomials from the NEW_TRAITS file

# Only keep the breeding season, non-breeding season and resident polygons
# 1 = resident; 2 = breeding; 3 = non-breeding
seabirds <- sb[sb$SEASONAL %in% c(1, 2, 3), ]

# Only keep species that are extant
# 1 = Extant 
seabirds <- seabirds[seabirds$PRESENCE %in% c(1), ]

# Only keep species that are native
# 1 = Native
seabirds <- seabirds[seabirds$ORIGIN %in% c(1), ]

# Create PAM. This take ~30-40 minutes for 341 species
# Returns latitude and longitude with PAM for each species 
# Lat long values retuned are the 1 degree cell's centroid
PAM <- lets.presab(seabirds, resol = 1, xmn = -180, xmx = 180, ymn = -90, ymx = 90, count = TRUE)

# Save PAM as a .rds file
saveRDS(PAM, "Polygon_PAM.rds")

```



## **3. Quantify fishing overlap and intensity for all species**

Here we quantify (1) overlap with fisheries activities - the extent to which speciesâ€™ distributions overlap with spatially-resolved gear-specific fishing and (2) fishing intensity - the intensity of fishing within the overlap regions. We use the Global Fishing Watch fishing effort data and the seabird presence-absence matrix to achieve this. To ensure consistency between the speciesâ€™ distribution and gear-specific fishing activity layers, we re-projected all spatial data to a raster format with the same coordinate reference system (WGS84), resolution (1Â° x 1Â° global grid cells) and extent (Â± 180Â°, Â± 90Â°). 

The fishing overlap and intensity values for each species are saved into a file called `fishing_overlap`.

```{r, eval = FALSE}
library(raster); library(tidyverse)

full_traits <- read.csv("341_sb_traits.csv")

# Read in the PAM
PAM <- readRDS("Polygon_PAM.rds")
PAM <- as.data.frame(PAM[["Presence_and_Absence_Matrix"]])

# Replace all zeros with NA
PAM[PAM == 0] <- NA

# Move the latitude and longitude columns to the final columns of the dataframe because of the loop below
Longitude <- PAM$`Longitude(x)`
Latitude <- PAM$`Latitude(y)`
PAM$`Longitude(x)`<- NULL
PAM$`Latitude(y)`<- NULL
PAM$`Longitude(x)`<- Longitude 
PAM$`Latitude(y)`<- Latitude

# Create a dataframe with all the species names to place the below calculations
fishing_overlap <- as.data.frame(full_traits$binomial); colnames(fishing_overlap) <- "binomial"
fishing_overlap$seine_percent_overlap <- NA
fishing_overlap$seine_sum_intensity <- NA
fishing_overlap$trawler_percent_overlap <- NA
fishing_overlap$trawler_sum_intensity <- NA
fishing_overlap$longline_percent_overlap <- NA
fishing_overlap$longline_sum_intensity <- NA

# Create a reference raster with 1 degree cells
# This will be used to reproject the fishing and species distribution .csv files to a raster with the same extent so that further analyses can be done 
ref <- raster(nrows=180, ncols=360, xmn=-180, xmx=180, ymn=-90, ymx=90)



### SEINE ###

# read in seine data
seine <- readRDS("SUM_seines.rds")

# Turn the seine csv into a raster based on the reference raster "ref". Sum the fishing hours per cell.
seine_raster <- rasterize(seine[, c('lon_bin', 'lat_bin')], ref, seine[, 'fishing_hours'], fun=sum)
plot(seine_raster)

# Run through all the species (this takes a couple of minutes)
# This takes the columns (1:341) from the PAM dataframe and pastes it in the rows (1:341) of the new fishing_overlap dataframe. Each corresponding column and row have matching species. 
# Note - some species do not overlap with fishing activity therefore it returns warming messages
# This code takes a couple of minutes to run
for (i in 1:341){

# Turn the PAM csv into a raster based on the reference raster
species_raster <- rasterize(PAM[, c('Longitude(x)', 'Latitude(y)')], ref, PAM[,i], fun=mean)

# Delete all cells that do not overlap
overlap <- mask(species_raster,  seine_raster)

# Calculate PERCENTAGE OVERLAP between species distributions and fishing activity
fishing_overlap$seine_percent_overlap[i] <- (sum(overlap@data@values, na.rm = TRUE)/sum(species_raster@data@values, na.rm = TRUE))*100

# Calculate the FISHING INTENSITY experienced 
# Delete all cells that do not overlap
intensity <- mask(seine_raster, species_raster)

# sum the number of overlapping cells
fishing_overlap$seine_sum_intensity[i] <- sum(intensity@data@values, na.rm = TRUE)
}

remove("seine", "seine_raster")

### TRAWLER ###

# read in trawler data
trawler <- readRDS("SUM_trawlers.rds")

# Turn the trawler csv into a raster based on the reference raster "ref". Sum the fishing hours per cell.
trawler_raster <- rasterize(trawler[, c('lon_bin', 'lat_bin')], ref, trawler[, 'fishing_hours'], fun=sum)
plot(trawler_raster)

# Run the same loop for the trawler data
for (i in 1:341){

# Turn the PAM csv into a raster based on the reference raster
species_raster <- rasterize(PAM[, c('Longitude(x)', 'Latitude(y)')], ref, PAM[,i], fun=mean)

# Delete all cells that do not overlap
overlap <- mask(species_raster,  trawler_raster)

# Calculate PERCENTAGE OVERLAP between species distributions and fishing activity
fishing_overlap$trawler_percent_overlap[i] <- (sum(overlap@data@values, na.rm = TRUE)/sum(species_raster@data@values, na.rm = TRUE))*100

# Calculate the FISHING INTENSITY experienced 
# Delete all cells that do not overlap
intensity <- mask(trawler_raster, species_raster)

# sum the number of overlapping cells
fishing_overlap$trawler_sum_intensity[i] <- sum(intensity@data@values, na.rm = TRUE)
}

remove("trawler", "trawler_raster")

### LONGLINE ###

# read in longline data
longline <- readRDS("SUM_longlines.rds")

# Turn the longline csv into a raster based on the reference raster "ref". Sum the fishing hours per cell.
longline_raster <- rasterize(longline[, c('lon_bin', 'lat_bin')], ref, longline[, 'fishing_hours'], fun=sum)
plot(longline_raster)

# Run the same loop for the longline data
for (i in 1:341){
  
  # Turn the PAM csv into a raster based on the reference raster
  species_raster <- rasterize(PAM[, c('Longitude(x)', 'Latitude(y)')], ref, PAM[,i], fun=mean)
  
  # Delete all cells that do not overlap
  overlap <- mask(species_raster,  longline_raster)
  
  # Calculate PERCENTAGE OVERLAP between species distributions and fishing activity
  fishing_overlap$longline_percent_overlap[i] <- (sum(overlap@data@values, na.rm = TRUE)/sum(species_raster@data@values, na.rm = TRUE))*100
  
  # Calculate the FISHING INTENSITY experienced 
  # Delete all cells that do not overlap
  intensity <- mask(longline_raster, species_raster)
  
  # sum the number of overlapping cells
  fishing_overlap$longline_sum_intensity[i] <- sum(intensity@data@values, na.rm = TRUE)
}

remove("longline", "longline_raster")

write.csv(fishing_overlap, "fishing_overlap.csv")


# Read in the csv again
fishing_overlap <- read.csv("fishing_overlap.csv")


```


## **4. Quantify gear-specific bycatch vulnerability**

Here we select four traits from Richards et al. (2020) - body mass, foraging guild, generation length and clutch size. We bin the continuous traits, fishing intensity and overlap with Sturges algorithm because the vulnerability framework requires categorical data. Finally, we build the vulnerability framework as described in Potter et al. (2017) *New Forests*, 48:275â€“300.

Each trait, attribute and dimension were scored between 0 - 1, with 1 indicating the greatest vulnerability to bycatch 

`traits.matrix` - All trait categories were then scored from high to low with ordinal variables based on increased vulnerability to bycatch. To ensure the prioritisation analysis predictably weights the criteria, all scores were scaled between zero and one and weighted by the frequency of trait occurrence.

The attribute score is the mean across all traits within each attribute, and the dimension score is the mean across all attributes. Total vulnerability is the mean score across all three dimensions. 


### *Longline Vulnerability*

```{r, eval = FALSE}

library(dplyr)

# Read in traits dataframe
traits <- read.csv("NEW_TRAITS_ENGLISH_Feb27_2020.csv")

# Delete columns that will not be used in this analysis
traits <- traits %>% select (-c(X, Order, Family, 
                                 English, hab_breadth,
                                 diet_5cat, pelagic_specialist, Threat, IUCN, migrate))

# longline overlap
traits$overlap <- fishing_overlap$longline_percent_overlap
traits$overlap[traits$overlap == "NaN"] <- 0

# Longline Intensity
traits$intensity <- fishing_overlap$longline_sum_intensity
traits$intensity[traits$intensity == "NaN"] <- 0


# Bin the continuous traits using Sturges Algorithm

library(dlookr);library(dplyr)

# function to bin data using sturges algorithm

cut_bin_stur <- function(x) {
  h <- hist(x, breaks = "Sturges")
  as.numeric(cut(x, breaks = h$breaks))
}

# bin data using sturges algorithm as the number of bins
traits <- traits %>% 
  mutate_at(vars(body_mass_median, GL, overlap, intensity, clutch), cut_bin_stur)

# Replace NAs in fishing intensity and overlap with 0
traits$overlap[is.na(traits$overlap)] <- 0
traits$intensity[is.na(traits$intensity)] <- 0
traits$clutch[is.na(traits$clutch)] <- 0


# setting up ready to transpose
rownames(traits) <- traits$binomial
traits$binomial <- NULL

# Transpose the traits dataframe
library(data.table);library(dplyr)
transp_traits <- transpose(traits)
colnames(transp_traits) <- rownames(traits)
transp_traits$Trait <- c( "Clutch", "BodyMass", "GenerationLength", "ForagingGuild","Overlap", "Intensity")

# reorder rows
x <- c("Overlap", "Intensity", "ForagingGuild", "BodyMass", "GenerationLength", "Clutch")
transp_traits <- transp_traits %>% slice(match(x, Trait))

# Add columns to match the trait.matrix
transp_traits$Attribute <- c("Range", "Magnitude","Feeding",  "Size", "Population", "Population") # add range for overlap
transp_traits$Category <- c("Exposure","Exposure", "Sensitivity", "Sensitivity", "Adaptive", "Adaptive")

# Reorder the columns
transp_traits <- transp_traits %>% select(c(Category, Attribute, Trait), everything())

traits.species.vulnerability.scores <- transp_traits



### QUANTIFY LONGLINE VULNERABILITY ###


# Read in the trait matrix
traits.matrix=read.csv("cerren-matrix-sturges-longline-noIUCN.csv")

# Extracting the numerical trait scores from the main datafile, and retaining the species order, NUMBERS 4 AND 7 RELATE TO THE SPECIES IN COLUMNS 4 TO 7
Tscores <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

# Make a numerical matrix to fill in the loop (with the correct dimensions)
Tscores.Num <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

for (i in 1: max(traits.matrix$UniqueID)) {
  match.dat=subset(traits.matrix,UniqueID==i)[,5:6]
  Tscores.Num[,i]<-match(Tscores[,i],match.dat$Level)
}

library(plyr)

# Extracting frequency of the modality for each trait, the elements [[]] in the following correspond to the rows in traits.species.vulnerability.scores
Tscores2<-apply(Tscores.Num,2,count)

# Extracting precentage of the modality for one trait/need to figure out across, correspond to the rows in traits.species.vulnerability.scores (for fun, consider doing as matrix)
Tscores3<-sapply(Tscores2, "[[", 2)
Tscores4<-lapply(Tscores3,sum)
Tscores5<-mapply("/",Tscores3,Tscores4,SIMPLIFY = FALSE)
Tperc=lapply(Tscores5,cumsum)
Tperc1 <- lapply(Tperc, function(i) { 
  i[1] <- 0
  return(i)
})

# Match Quant.Scores to Vuln.Scores and Traits Matrix (MUST HAVE ORDERS SCORES IN INPUT TRAITS MATRIX (traits.matrix) FROM LOW TO HIGH)
Tperc.to.match<-melt(Tperc1)

# Bind traits matrix to Quant Scores
# Data deficient species might make a missmatch here if using 341 sp
traits.matrix.quant <- cbind(traits.matrix,Tperc.to.match$value)
colnames(traits.matrix.quant)<-c("Category","Attribute","Trait","UniqueID","Level","Vuln.Score","Quant.Score")

# Reshape traits by species matrix to get out
species.names <- colnames(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])
traits.species.vulnerability.scores.reshape<-reshape(traits.species.vulnerability.scores,varying = list(species.names),times = species.names,direction="long")

Quant.Species.Vuln=traits.species.vulnerability.scores.reshape[,-6]
colnames(Quant.Species.Vuln)<-c("Category","Attribute","Trait","Species","Level")

Vuln.Quant.Data=merge(Quant.Species.Vuln,traits.matrix.quant,by.x=c("Category","Attribute","Trait","Level"),all = TRUE)

# This is where we can take averages, and you could add in multiple modes here (by including "Trait" after Attribute - this would allow an average across multiple Levels of a trait)
species.total.V.Attribute.longline=ddply(Vuln.Quant.Data,c("Species","Category","Attribute"),summarize, Quant.Score.Mean.Attribute=mean(Quant.Score))

species.total.V.Category.longline=ddply(species.total.V.Attribute.longline,c("Species","Category"),summarize, Quant.Score.Mean.Category=mean(Quant.Score.Mean.Attribute))

# mean vulnerability
species.total.V.longline=ddply(species.total.V.Category.longline,c("Species"),summarize, Quant.Score.V.all=mean(Quant.Score.Mean.Category))

library(ggplot2)
# Plot species by vulnerability in each category
qplot(data=species.total.V.Category.longline,Category, Quant.Score.Mean.Category)

```










### *Trawl Vulnerability*

```{r, eval = FALSE}

library(dplyr)

# Read in traits dataframe
traits <- read.csv("NEW_TRAITS_ENGLISH_Feb27_2020.csv")

# Delete columns that will not be used in this analysis
traits <- traits %>% select (-c(X, Order, Family, 
                                 English, hab_breadth,
                                 diet_5cat, pelagic_specialist, IUCN, Threat, migrate))

# longline overlap
traits$overlap <- fishing_overlap$trawler_percent_overlap
traits$overlap[traits$overlap == "NaN"] <- 0

# Longline Intensity
traits$intensity <- fishing_overlap$trawler_sum_intensity
traits$intensity[traits$intensity == "NaN"] <- 0


# Bin the continuous traits using Sturges Algorithm

library(dlookr);library(dplyr)

# function to bin data using sturges algorithm

cut_bin_stur <- function(x) {
  h <- hist(x, breaks = "Sturges")
  as.numeric(cut(x, breaks = h$breaks))
}

# bin data using sturges algorithm as the number of bins
traits <- traits %>% 
  mutate_at(vars(body_mass_median, GL, overlap, intensity, clutch), cut_bin_stur)

# Replace NAs in fishing intensity and overlap with 0
traits$overlap[is.na(traits$overlap)] <- 0
traits$intensity[is.na(traits$intensity)] <- 0
traits$clutch[is.na(traits$clutch)] <- 0



# count number of bins for each column:
unique(traits$clutch)
#clutch = 7/6
#body mass: 8
#GL: 9 (goes up to 10, but excludes 9...?)
# overlap = 10 +NA
# intensity = 11+NA


# setting up ready to transpose
rownames(traits) <- traits$binomial
traits$binomial <- NULL

# Transpose the traits dataframe
library(data.table);library(dplyr)
transp_traits <- transpose(traits)
colnames(transp_traits) <- rownames(traits)
transp_traits$Trait <- c( "Clutch", "BodyMass", "GenerationLength", "ForagingGuild", "Overlap", "Intensity")

# reorder rows
x <- c("Overlap", "Intensity", "ForagingGuild", "BodyMass", "GenerationLength", "Clutch")
transp_traits <- transp_traits %>% slice(match(x, Trait))

# Add columns to match the trait.matrix
transp_traits$Attribute <- c("Range", "Magnitude","Feeding",  "Size", "Population", "Population") # add range for overlap
transp_traits$Category <- c("Exposure","Exposure","Sensitivity", "Sensitivity", "Adaptive", "Adaptive")

# Reorder the columns
transp_traits <- transp_traits %>% select(c(Category, Attribute, Trait), everything())

traits.species.vulnerability.scores <- transp_traits



### QUANTIFY trawler VULNERABILITY ###



# Read in the trait matrix
traits.matrix=read.csv("cerren-matrix-sturges-trawler-noIUCN.csv")

# Extracting the numerical trait scores from the main datafile, and retaining the species order, NUMBERS 4 AND 7 RELATE TO THE SPECIES IN COLUMNS 4 TO 7
Tscores <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

# Make a numerical matrix to fill in the loop (with the correct dimensions)
Tscores.Num <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

for (i in 1: max(traits.matrix$UniqueID)) {
  match.dat=subset(traits.matrix,UniqueID==i)[,5:6]
  Tscores.Num[,i]<-match(Tscores[,i],match.dat$Level)
}

# Extracting frequency of the modality for each trait, the elements [[]] in the following correspond to the rows in traits.species.vulnerability.scores
Tscores2<-apply(Tscores.Num,2,count)

# Extracting precentage of the modality for one trait/need to figure out across, correspond to the rows in traits.species.vulnerability.scores (for fun, consider doing as matrix)
Tscores3<-sapply(Tscores2, "[[", 2)
Tscores4<-lapply(Tscores3,sum)
Tscores5<-mapply("/",Tscores3,Tscores4,SIMPLIFY = FALSE)
Tperc=lapply(Tscores5,cumsum)
Tperc1 <- lapply(Tperc, function(i) { 
  i[1] <- 0
  return(i)
})

# Match Quant.Scores to Vuln.Scores and Traits Matrix (MUST HAVE ORDERS SCORES IN INPUT TRAITS MATRIX (traits.matrix) FROM LOW TO HIGH)
Tperc.to.match<-melt(Tperc1)

# Bind traits matrix to Quant Scores
# Data deficient species might make a missmatch here if using 341 sp
traits.matrix.quant <- cbind(traits.matrix,Tperc.to.match$value)
colnames(traits.matrix.quant)<-c("Category","Attribute","Trait","UniqueID","Level","Vuln.Score","Quant.Score")

# Reshape traits by species matrix to get out
species.names <- colnames(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])
traits.species.vulnerability.scores.reshape<-reshape(traits.species.vulnerability.scores,varying = list(species.names),times = species.names,direction="long")

Quant.Species.Vuln=traits.species.vulnerability.scores.reshape[,-6]
colnames(Quant.Species.Vuln)<-c("Category","Attribute","Trait","Species","Level")

Vuln.Quant.Data=merge(Quant.Species.Vuln,traits.matrix.quant,by.x=c("Category","Attribute","Trait","Level"),all = TRUE)

# This is where we can take averages, and you could add in multiple modes here (by including "Trait" after Attribute - this would allow an average across multiple Levels of a trait)
species.total.V.Attribute.trawler=ddply(Vuln.Quant.Data,c("Species","Category","Attribute"),summarize, Quant.Score.Mean.Attribute=mean(Quant.Score))

species.total.V.Category.trawler=ddply(species.total.V.Attribute.trawler,c("Species","Category"),summarize, Quant.Score.Mean.Category=mean(Quant.Score.Mean.Attribute))

# mean vulnerability
species.total.V.trawler=ddply(species.total.V.Category.trawler,c("Species"),summarize, Quant.Score.V.all=mean(Quant.Score.Mean.Category))


# Plot species by vulnerability in each category
qplot(data=species.total.V.Category.trawler,Category, Quant.Score.Mean.Category)

```












### *Purse Seine Vulnerability*

```{r, eval = FALSE}

library(dplyr)

# Read in traits dataframe
traits <- read.csv("NEW_TRAITS_ENGLISH_Feb27_2020.csv")

# Delete columns that will not be used in this analysis
traits <- traits %>% select (-c(X, Order, Family, 
                                 English, hab_breadth,
                                 diet_5cat, pelagic_specialist, IUCN, Threat, migrate))

# longline overlap
traits$overlap <- fishing_overlap$seine_percent_overlap
traits$overlap[traits$overlap == "NaN"] <- 0

# Longline Intensity
traits$intensity <- fishing_overlap$seine_sum_intensity
traits$intensity[traits$intensity == "NaN"] <- 0


# Bin the continuous traits using Sturges Algorithm

library(dlookr);library(dplyr)

# function to bin data using sturges algorithm

cut_bin_stur <- function(x) {
  h <- hist(x, breaks = "Sturges")
  as.numeric(cut(x, breaks = h$breaks))
}

# bin data using sturges algorithm as the number of bins
traits <- traits %>% 
  mutate_at(vars(body_mass_median, GL, overlap, intensity, clutch), cut_bin_stur)

# Replace NAs in fishing intensity and overlap with 0
traits$overlap[is.na(traits$overlap)] <- 0
traits$intensity[is.na(traits$intensity)] <- 0
traits$clutch[is.na(traits$clutch)] <- 0


# count number of bins for each column:
unique(traits$clutch)
#clutch = 7/6
#body mass: 8
#GL: 9 (goes up to 10, but excludes 9...?)
# overlap = 10 +NA
# intensity = 11+NA


# setting up ready to transpose
rownames(traits) <- traits$binomial
traits$binomial <- NULL

# Transpose the traits dataframe
library(data.table);library(dplyr)
transp_traits <- transpose(traits)
colnames(transp_traits) <- rownames(traits)
transp_traits$Trait <- c( "Clutch", "BodyMass", "GenerationLength", "ForagingGuild", "Overlap", "Intensity")

# reorder rows
x <- c("Overlap", "Intensity", "ForagingGuild",  "BodyMass", "GenerationLength", "Clutch")
transp_traits <- transp_traits %>% slice(match(x, Trait))

# Add columns to match the trait.matrix
transp_traits$Attribute <- c("Range", "Magnitude","Feeding",  "Size", "Population", "Population") # add range for overlap
transp_traits$Category <- c("Exposure","Exposure","Sensitivity", "Sensitivity",  "Adaptive", "Adaptive")

# Reorder the columns
transp_traits <- transp_traits %>% select(c(Category, Attribute, Trait), everything())

traits.species.vulnerability.scores <- transp_traits



### QUANTIFY seine VULNERABILITY ###



# Read in the trait matrix
traits.matrix=read.csv("cerren-matrix-sturges-seine-noIUCN.csv")

# Extracting the numerical trait scores from the main datafile, and retaining the species order, NUMBERS 4 AND 7 RELATE TO THE SPECIES IN COLUMNS 4 TO 7
Tscores <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

# Make a numerical matrix to fill in the loop (with the correct dimensions)
Tscores.Num <- t(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])

for (i in 1: max(traits.matrix$UniqueID)) {
  match.dat=subset(traits.matrix,UniqueID==i)[,5:6]
  Tscores.Num[,i]<-match(Tscores[,i],match.dat$Level)
}

# Extracting frequency of the modality for each trait, the elements [[]] in the following correspond to the rows in traits.species.vulnerability.scores
Tscores2<-apply(Tscores.Num,2,count)

# Extracting precentage of the modality for one trait/need to figure out across, correspond to the rows in traits.species.vulnerability.scores (for fun, consider doing as matrix)
Tscores3<-sapply(Tscores2, "[[", 2)
Tscores4<-lapply(Tscores3,sum)
Tscores5<-mapply("/",Tscores3,Tscores4,SIMPLIFY = FALSE)
Tperc=lapply(Tscores5,cumsum)
Tperc1 <- lapply(Tperc, function(i) { 
  i[1] <- 0
  return(i)
})

# Match Quant.Scores to Vuln.Scores and Traits Matrix (MUST HAVE ORDERS SCORES IN INPUT TRAITS MATRIX (traits.matrix) FROM LOW TO HIGH)
Tperc.to.match<-melt(Tperc1)

# Bind traits matrix to Quant Scores
# Data deficient species might make a missmatch here if using 341 sp
traits.matrix.quant <- cbind(traits.matrix,Tperc.to.match$value)
colnames(traits.matrix.quant)<-c("Category","Attribute","Trait","UniqueID","Level","Vuln.Score","Quant.Score")

# Reshape traits by species matrix to get out
species.names <- colnames(traits.species.vulnerability.scores[,4:dim(traits.species.vulnerability.scores)[2]])
traits.species.vulnerability.scores.reshape<-reshape(traits.species.vulnerability.scores,varying = list(species.names),times = species.names,direction="long")

Quant.Species.Vuln=traits.species.vulnerability.scores.reshape[,-6]
colnames(Quant.Species.Vuln)<-c("Category","Attribute","Trait","Species","Level")

Vuln.Quant.Data=merge(Quant.Species.Vuln,traits.matrix.quant,by.x=c("Category","Attribute","Trait","Level"),all = TRUE)

# This is where we can take averages, and you could add in multiple modes here (by including "Trait" after Attribute - this would allow an average across multiple Levels of a trait)
species.total.V.Attribute.seine=ddply(Vuln.Quant.Data,c("Species","Category","Attribute"),summarize, Quant.Score.Mean.Attribute=mean(Quant.Score))

species.total.V.Category.seine=ddply(species.total.V.Attribute.seine,c("Species","Category"),summarize, Quant.Score.Mean.Category=mean(Quant.Score.Mean.Attribute))

# mean vulnerability
species.total.V.seine=ddply(species.total.V.Category.seine,c("Species"),summarize, Quant.Score.V.all=mean(Quant.Score.Mean.Category))


# Plot species by vulnerability in each category
qplot(data=species.total.V.Category.seine,Category, Quant.Score.Mean.Category)

```









### *Building vulnerability score dataframes*

Here we build a dataframe for each gear type containing the vulnerability scores for the Sensitivity, Adaptive, and Exposure dimensions, in addition to the overall vulnerability score.

```{r, eval = FALSE}

library(tidyr)

traits <- read.csv("NEW_TRAITS_ENGLISH_Feb27_2020.csv")

#subset species info
species_id <- traits %>% select(binomial, English, Order)
colnames(species_id)[colnames(species_id) == 'binomial'] <- 'Species'

### SEINE ###

# Reorder the categories 
seine.categories<- species.total.V.Category.seine %>% 
  spread(Category, Quant.Score.Mean.Category, fill = 0)

# add the total vulnerability
seine.categories$Vulnerability <- species.total.V.seine$Quant.Score.V.all

# add a gear-type name
seine.categories$Gear <- "Purse Seine"

# arrange in decending order
seine.categories <- seine.categories[order(-seine.categories$Vulnerability),]

# add the species info
seine.categories <- left_join(seine.categories, species_id, by = "Species")

# reorder the columns
seine.categories <- seine.categories %>% select(Gear, Species, English, Order, Sensitivity, Adaptive, Exposure, Vulnerability)

write.csv(seine.categories, "Total_Vulnerability_Seine.csv")
write.csv(seine.categories, "Total_Vulnerability_Seine_THIS.csv")
write.csv(seine.categories, "Total_Vulnerability_Seine_noIUCN.csv")

### LONGLINE ###

# Reorder the categories 
longline.categories<- species.total.V.Category.longline %>% 
  spread(Category, Quant.Score.Mean.Category, fill = 0)

# add the total vulnerability
longline.categories$Vulnerability <- species.total.V.longline$Quant.Score.V.all

# add a gear-type name
longline.categories$Gear <- "Longline"

# arrange in decending order
longline.categories <- longline.categories[order(-longline.categories$Vulnerability),]

# add the species info
longline.categories <- left_join(longline.categories, species_id, by = "Species")

# reorder the columns
longline.categories <- longline.categories %>% select(Gear, Species, English, Order, Sensitivity, Adaptive, Exposure, Vulnerability)

write.csv(longline.categories, "Total_Vulnerability_Longline.csv")
write.csv(longline.categories, "Total_Vulnerability_Longline_noIUCN.csv")


### TRAWLER ###

# Reorder the categories 
trawler.categories<- species.total.V.Category.trawler %>% 
  spread(Category, Quant.Score.Mean.Category, fill = 0)

# add the total vulnerability
trawler.categories$Vulnerability <- species.total.V.trawler$Quant.Score.V.all

# add a gear-type name
trawler.categories$Gear <- "Trawl"

# arrange in decending order
trawler.categories <- trawler.categories[order(-trawler.categories$Vulnerability),]

# add the species info
trawler.categories <- left_join(trawler.categories, species_id, by = "Species")

# reorder the columns
trawler.categories <- trawler.categories %>% select(Gear, Species, English, Order, Sensitivity, Adaptive, Exposure, Vulnerability)

write.csv(trawler.categories, "Total_Vulnerability_Trawler.csv")
write.csv(trawler.categories, "Total_Vulnerability_Trawler_noIUCN.csv")

```






## **5. Importing species' threat data from the IUCN**

To compare the species identified as vulnerable to bycatch by the IUCN and the vulnerability framework, we extract the IUCN Red List threats. IUCN Threat Data were downloaded on July 14th, 2020. Because threats are updated through time, they may change in the future.

Note - an API key is needed to download the IUCN data.

```{r message=FALSE, error=FALSE, warning=FALSE, eval = FALSE}

library(rredlist); library(rlist) # Load package

spp <- as.character(traits$binomial) # vector of species names from the traits dataframe

iucn_key <- " " # Enter your unique IUCN code here


# Extract the threats for each species from th IUCN database. 
# NOTE - this code take ~15 minutes to run.

iucn <- lapply(spp, function(x) {
  y <- rl_threats(name = x, key = iucn_key)
  Sys.sleep(2)
  # 2 second delay makes API work better - recommended by IUCN
  return(y)
})

### Add an extra column to the dataframes in the list with the binomial names:
for (i in 1:length(iucn)) { 
  iucn[[i]][["result"]]$binomial <- iucn[[i]][["name"]] 
}

iucn <- Reduce(rbind, iucn) ## Reduce the lists down
iucn[1:341] <- NULL # remove all of the extra species names in the list

# Create an alternative dataframe for species with no threats (n = 36)
no_threats <- iucn[sapply(iucn, length)<2]
no_threats <- list.rbind(no_threats) # binds all list elements by row to make a dataframe
no_threats <- list.rbind(no_threats) # run twice
colnames(no_threats) <- "binomial"
no_threats <- as.data.frame(no_threats)
no_threats$binomial <- as.character(no_threats$binomial)
no_threats$code <- as.character(0)
no_threats$title <- "No threats to population"


# Remove all lists with species with no threats
iucn <- iucn[sapply(iucn, length)>1] 
# binds all list elements by row to make a dataframe
iucn <- list.rbind(iucn) 

# Delete the columns that will not be used in further analyses
iucn <- iucn[- c(3:7)] 

# match the no threats dataframe with the iucn dataframe
IUCN_threats <- rbind(iucn, no_threats)

# Rename threats to their general threats 
IUCN_threats$Threat_Class <- as.character(IUCN_threats$code) #make new column
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "0")] <- "No threats"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "1.")] <- "Residential & commercial development"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "2.")] <- "Agriculture & aquaculture"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "3.")] <- "Energy production & mining"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "4.")] <- "Transportation & service corridors"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "5.")] <- "Biological resource use"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "6.")] <- "Human intrusions & disturbance"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "7.")] <- "Natural system modifications"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "8.")] <- "Invasive & native species, genes & diseases"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "9.")] <- "Pollution"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "10.")] <- "Geological events"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "11.")] <- "Climate change & severe weather"
IUCN_threats$Threat_Class[startsWith(IUCN_threats$Threat_Class, "12.")] <- "Other"


```


### *Subset species at risk to bycatch*

Codes = https://www.iucnredlist.org/resources/threat-classification-scheme

```{r, eval = FALSE}

bycatch <- read.csv("CERREN THIS THREAT FILE.csv")

# subset based on fisheries activity codes 
bycatch <- bycatch[bycatch$code %in% c("5.4.3", "5.4.4"), ]
#bycatch$binomial <- gsub(" ", ".", bycatch$binomial)


bycatch <- bycatch[!duplicated(bycatch$binomial), ] #remove duplicates
bycatch <- as.data.frame(bycatch$binomial); colnames(bycatch) <- "Species"
bycatch$bycatch <- 1


```



## **6. Assigning species to vulnerability classes**

We categorise species into five vulnerability classes based on a dimension score threshold of 55%.

- **High Vulnerability** - exposure, sensitivity, & adaptive capacity dimensions have a score >= 55%
- **Potential Adapters** - sensitivity & exposure scores are >= 55%, but adaptive capacity is < 55%
- **Potential Persisters** - adaptive capacity & exposure scores are >= 55%, but sensitivity is < 55%
- **Potential Future Vulnerability** - adaptive capacity & sensitivity scores are >= 55%, but exposure is < 55%
- **Low Vulnerability** - all dimensions have a score < 55%, or only one dimension has a score >= 55%


```{r, eval = FALSE}

### LONGLINE ###

longline.categories$class <- NA

# Category 5 (Low Vulnerability)
longline.categories$class[longline.categories$Sensitivity < 0.55 & longline.categories$Adaptive < 0.55 & longline.categories$Exposure < 0.55] <- "Low Vulnerability"
longline.categories$class[longline.categories$Sensitivity < 0.55 & longline.categories$Adaptive < 0.55 & longline.categories$Exposure >= 0.55] <- "Low Vulnerability"
longline.categories$class[longline.categories$Sensitivity < 0.55 & longline.categories$Adaptive >= 0.55 & longline.categories$Exposure < 0.55] <- "Low Vulnerability" 
longline.categories$class[longline.categories$Sensitivity >= 0.55 & longline.categories$Adaptive < 0.55 & longline.categories$Exposure < 0.55] <- "Low Vulnerability" 

# Category 1 (High vulnerability)
longline.categories$class[longline.categories$Sensitivity >= 0.55 & longline.categories$Adaptive >= 0.55 & longline.categories$Exposure >= 0.55] <-  "High Vulnerability"

# Category 2 (High vulnerability, potential adaptation)
longline.categories$class[longline.categories$Sensitivity >= 0.55 & longline.categories$Adaptive < 0.55 & longline.categories$Exposure >= 0.55] <- "Potential Adapters"

# Category 3 (High vulnerability, potential persistance)
longline.categories$class[longline.categories$Sensitivity < 0.55 & longline.categories$Adaptive >= 0.55 & longline.categories$Exposure >= 0.55] <- "Potential Persisters"

# Category 4 (Potential high future vulnerability)
longline.categories$class[longline.categories$Sensitivity >= 0.55 & longline.categories$Adaptive >= 0.55 & longline.categories$Exposure < 0.55] <- "Potential Future Vulnerability"


# Add the IUCN bycatch species column
longline.categories <- left_join(longline.categories, bycatch, by = "Species")
longline.categories$bycatch[is.na(longline.categories$bycatch)] <- 0

# Save
write.csv(longline.categories, "Total_Vulnerability_Longline_bycatch.csv")
write.csv(longline.categories, "Total_Vulnerability_Longline_bycatch_noIUCN.csv")


# Calculate the mean
longline.mean<- longline.categories %>% 
  group_by(class) %>% 
  summarise(n=n(),
            MeanSensitivity = mean(Sensitivity), # mean dimention per class
            MeanAdaptive = mean(Adaptive),
            MeanExposure = mean(Exposure), 
            MeanVulnerability = mean(Vulnerability),
            IUCN = sum(bycatch)) 


#reorder the categories
longline.mean <- longline.mean %>%
  mutate(class =  factor(class, levels = c("High Vulnerability", 
                                           "Potential Adapters", 
                                           "Potential Persisters", 
                                           "Potential Future Vulnerability",
                                           "Low Vulnerability"))) %>%
  arrange(class)



### TRAWLER ###
trawler.categories$class <- NA

# Category 5 (Low Vulnerability)
trawler.categories$class[trawler.categories$Sensitivity < 0.55 & trawler.categories$Adaptive < 0.55 & trawler.categories$Exposure < 0.55] <- "Low Vulnerability"
trawler.categories$class[trawler.categories$Sensitivity < 0.55 & trawler.categories$Adaptive < 0.55 & trawler.categories$Exposure >= 0.55] <- "Low Vulnerability"
trawler.categories$class[trawler.categories$Sensitivity < 0.55 & trawler.categories$Adaptive >= 0.55 & trawler.categories$Exposure < 0.55] <- "Low Vulnerability"
trawler.categories$class[trawler.categories$Sensitivity >= 0.55 & trawler.categories$Adaptive < 0.55 & trawler.categories$Exposure < 0.55] <- "Low Vulnerability"

# Category 1 (High vulnerability)
trawler.categories$class[trawler.categories$Sensitivity >= 0.55 & trawler.categories$Adaptive >= 0.55 & trawler.categories$Exposure >= 0.55] <-  "High Vulnerability"

# Category 2 (High vulnerability, potential adaptation)
trawler.categories$class[trawler.categories$Sensitivity >= 0.55 & trawler.categories$Adaptive < 0.55 & trawler.categories$Exposure >= 0.55] <- "Potential Adapters"

# Category 3 (High vulnerability, potential persistance)
trawler.categories$class[trawler.categories$Sensitivity < 0.55 & trawler.categories$Adaptive >= 0.55 & trawler.categories$Exposure >= 0.55] <- "Potential Persisters"

# Category 4 (Potential high future vulnerability)
trawler.categories$class[trawler.categories$Sensitivity >= 0.55 & trawler.categories$Adaptive >= 0.55 & trawler.categories$Exposure < 0.55] <- "Potential Future Vulnerability"

# IUCN bycatch species
trawler.categories <- left_join(trawler.categories, bycatch, by = "Species")
trawler.categories$bycatch[is.na(trawler.categories$bycatch)] <- 0

write.csv(trawler.categories, "Total_Vulnerability_Trawler_bycatch.csv")
write.csv(trawler.categories, "Total_Vulnerability_Trawler_bycatch_noIUCN.csv")

# means
trawler.mean<-trawler.categories %>% 
  group_by(class) %>% 
  summarise(n=n(),
            MeanSensitivity = mean(Sensitivity), # mean dimention per class
            MeanAdaptive = mean(Adaptive),
            MeanExposure = mean(Exposure), 
            MeanVulnerability = mean(Vulnerability),
            IUCN = sum(bycatch)) 

#reorder the categories
trawler.mean <- trawler.mean %>%
  mutate(class =  factor(class, levels = c("High Vulnerability", 
                                           "Potential Adapters", 
                                           "Potential Persisters", 
                                           "Potential Future Vulnerability",
                                           "Low Vulnerability"))) %>%
  arrange(class)



### SEINE ###

seine.categories$class <- NA

# Category 5 (Low Vulnerabilit "Low Vulnerability"
seine.categories$class[seine.categories$Sensitivity < 0.55 & seine.categories$Adaptive < 0.55 & seine.categories$Exposure < 0.55] <-  "Low Vulnerability"
seine.categories$class[seine.categories$Sensitivity < 0.55 & seine.categories$Adaptive < 0.55 & seine.categories$Exposure >= 0.55] <- "Low Vulnerability"
seine.categories$class[seine.categories$Sensitivity < 0.55 & seine.categories$Adaptive >= 0.55 & seine.categories$Exposure < 0.55] <- "Low Vulnerability"
seine.categories$class[seine.categories$Sensitivity >= 0.55 & seine.categories$Adaptive < 0.55 & seine.categories$Exposure < 0.55] <- "Low Vulnerability"

# Category 1 (High vulnerability)
seine.categories$class[seine.categories$Sensitivity >= 0.55 & seine.categories$Adaptive >= 0.55 & seine.categories$Exposure >= 0.55] <-  "High Vulnerability"

# Category 2 (High vulnerability, potential adaptation)
seine.categories$class[seine.categories$Sensitivity >= 0.55 & seine.categories$Adaptive < 0.55 & seine.categories$Exposure >= 0.55] <- "Potential Adapters"

# Category 3 (High vulnerability, potential persistance)
seine.categories$class[seine.categories$Sensitivity < 0.55 & seine.categories$Adaptive >= 0.55 & seine.categories$Exposure >= 0.55] <- "Potential Persisters"

# Category 4 (Potential high future vulnerability)
seine.categories$class[seine.categories$Sensitivity >= 0.55 & seine.categories$Adaptive >= 0.55 & seine.categories$Exposure < 0.55] <- "Potential Future Vulnerability"

# IUCN bycatch species
seine.categories <- left_join(seine.categories, bycatch, by = "Species")
seine.categories$bycatch[is.na(seine.categories$bycatch)] <- 0

write.csv(seine.categories, "Total_Vulnerability_Seine_bycatch.csv")
write.csv(seine.categories, "Total_Vulnerability_Seine_bycatch_noIUCN.csv")


# mean
seine.mean <- seine.categories %>% 
  group_by(class) %>% 
  summarise(n=n(),
            MeanSensitivity = mean(Sensitivity), # mean dimention per class
            MeanAdaptive = mean(Adaptive),
            MeanExposure = mean(Exposure), 
            MeanVulnerability = mean(Vulnerability),
            IUCN = sum(bycatch))


#reorder the categories
seine.mean <- seine.mean %>%
  mutate(class =  factor(class, levels = c("High Vulnerability", 
                                           "Potential Adapters", 
                                           "Potential Persisters", 
                                           "Potential Future Vulnerability",
                                           "Low Vulnerability"))) %>%
  arrange(class)


```





## **7. Create tables and figures**

### *Table for mean vulnerability scores* 

```{r, eval = FALSE}

longline.mean$Gear <- "Longline"
trawler.mean$Gear <- "Trawl"
seine.mean$Gear <- "Purse Seine"

table.mean <- bind_rows(longline.mean, trawler.mean, seine.mean)
table.mean <- table.mean  %>% select(Gear, class, n, IUCN, everything())
colnames(table.mean) <- c("Gear Type", "Vulnerability Class", "Species (n)", "IUCN (n)", "Mean Sensitivity", "Mean Adaptive", "Mean Exposure", "Mean Vulnerability")

# change the significant figures
table.mean$`Mean Adaptive` <- signif(table.mean$`Mean Adaptive`, digits = 2)
table.mean$`Mean Sensitivity` <- signif(table.mean$`Mean Sensitivity`, digits = 2)
table.mean$`Mean Exposure` <- signif(table.mean$`Mean Exposure`, digits = 2)
table.mean$`Mean Vulnerability` <- signif(table.mean$`Mean Vulnerability`, digits = 2)

library(formattable);library(kableExtra)

colfunc <- colorRampPalette(c("orange", "light blue"))
colfunc(5)

color_bar_factor <- formatter("span",
                             style = function(x) style(
                               display = "block",
                               border.radius = "4px",
                               background = c("#FFA500","#D6BE73", "#C1CBAC" ,"#ADD8E6",
                                              "#FFA500" ,"#EAB139","#D6BE73", "#C1CBAC" ,"#ADD8E6",
                                              "#FFA500" ,"#EAB139","#D6BE73", "#C1CBAC" ,"#ADD8E6")))

#"High Vulnerability"                 = "#FFA500"         
#"Potential Adapters",                = "#EAB139"
#"Potential Persisters"               = "#D6BE73"
#"Potential Future Vulnerability",    = "#C1CBAC"           
#"Low Vulnerability"                  = "#ADD8E6"


formattable(table.mean, align =c("l","c","c","c", "c", "c", "c", "c"),
            list(`Vulnerability Class`=color_bar_factor))



```


### *Table for top five most vulnerable species*

```{r, eval = FALSE}


# subset the seines for only those species falling into the High Vulnerability class
seine.class1 <- filter(seine.categories, class == "High Vulnerability")

## Create a table with the top five for all combined

table <- bind_rows(longline.categories[1:5,], trawler.categories[1:5,], seine.class1[1:5,])
table$X <- NULL

# change the significant figures
table$Adaptive <- signif(table$Adaptive, digits = 2)
table$Sensitivity <- signif(table$Sensitivity, digits = 2)
table$Exposure <- signif(table$Exposure, digits = 2)
table$Vulnerability <- signif(table$Vulnerability, digits = 2)

#reorder
table <- table  %>% select(Gear, Species, English, Order, bycatch, everything())
#rename
colnames(table) <- c("Gear Type", "Species Name", "Common Name", "Order", "IUCN","Sensitivity", "Adaptive", "Exposure", "Vulnerability", "Vulnerability Class")

# assign TRUE/FALSE to IUCN column
table$IUCN[table$IUCN == 1] <- "TRUE"
table$IUCN[table$IUCN == 0] <- "FALSE"

library(formattable);library(kableExtra); library(tables)

color_bar_factor <- formatter("span",
                             style = function(x) style(
                               display = "block",
                               border.radius = "4px",
                               background = c("#FFA500","#EAB139", "#C1CBAC")[factor(as.character(x))]))


formattable(table, 
            align =c("l","l","l","l","c", "c", "c", "c", "c", "c"),
            list(
              `Species Name` = formatter("span", style = ~ style(font.style = "italic")),
              area(col = 9) ~ color_tile("light blue", "orange"), 
                 IUCN = formatter("span",style = x ~ style(color = ifelse(x, "green", "red")),  
                            x ~ icontext(ifelse(x, "ok", "remove"), ifelse(x, "Yes", "No"))),
              `Vulnerability Class`=color_bar_factor))

```


### *Figure for Sensitivity analysis*

These values were obtained by changing the percentage threshold from coding step 6: "Assigning species to vulnerability classes", and summing the number of species that fell into the "High Vulnerability" class.
```{r, eval = FALSE}

Percentage <- c(50, 55, 60, 65,
                50, 55, 60, 65,
                50, 55, 60, 65)

# The number of species was determines by changing the threshold in the code above
# and extracting the number of species falling into the "high vulnerability" class

Species <- c(52, 37, 30, 26,
             16, 12, 10, 3,
             19, 10, 0, 0)

#Species <- c(54, 37, 30, 24,
#             24, 12, 11, 2,
#             30, 15, 0, 0)

Gear <- c("Longline","Longline","Longline","Longline",
          "Trawl","Trawl","Trawl","Trawl",
          "Purse Seine", "Purse Seine","Purse Seine","Purse Seine" )

X <- as.data.frame(cbind(Species, Percentage, Gear))

X$Species <- as.numeric(as.character(X$Species))
X$Percentage <- as.numeric(as.character(X$Percentage))

library(ggplot2)

ggplot(data = X, aes(Percentage, Species)) +
  geom_line( size = 1) +
  geom_point() + 
  facet_wrap(~ Gear)+
  labs(y = "Number of Species", x = "Percentage (%)", 
       subtitle = "Sensitivity test for the change in number of species per percentage threshold") +
  theme_bw(base_size = 18)+
  theme(panel.grid.major = element_blank())
```

## IDENTIFY DIFFERENCE BETWEEN VULNERABILITY FRAMEWORK AND THE IUCN
```{r}
####________________________
## IDENTIFY DIFFERENCE BETWEEN VULNERABILITY FRAMEWORK AND THE IUCN

library(dplyr)
trawler.categories <- read.csv("Total_Vulnerability_Trawler_bycatch_noIUCN.csv")
longline.categories <- read.csv("Total_Vulnerability_Longline_bycatch_noIUCN.csv")
seine.categories <- read.csv("Total_Vulnerability_Seine_bycatch_noIUCN.csv")


## grouping the number of IUCN together from the gears
all <- bind_rows(longline.categories, trawler.categories, seine.categories)


# remove duplicates of species if they occur in the same vuln class
#samp2 <- all %>% group_by (Species, Vulnerability.Class) %>% arrange(Vulnerability.Class) %>% slice(1)
samp2 <- all %>% group_by (Species, class) %>% arrange(class) %>% slice(1)


# NOTE - MAY NEED TO switch between IUCN and bycatch
iucn_table<- samp2 %>% 
  group_by(class) %>% 
  summarise(n=n(),
            IUCN = sum(bycatch)) %>%
  mutate(Agreeing = (IUCN / n)*100) %>%
  mutate(Missing = 100 - Agreeing )


iucn_table <- iucn_table %>%
  mutate(class =  factor(class, levels = c("High Vulnerability", 
                                                                       "Potential Adapters", 
                                                                       "Potential Persisters", 
                                                                       "Potential Future Vulnerability",
                                                                       "Low Vulnerability"))) %>%
  arrange(class)
```


